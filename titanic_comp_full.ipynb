{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Kaggle Titanic Survival Competition**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import scipy.stats as stats\n",
    "import sklearn as sk\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Table of content**\n",
    "\n",
    "1. Exploring the Dataset\n",
    "   1.  Column Identification and Count\n",
    "   2.  Data Snapshot\n",
    "   3.  Data Types Overview\n",
    "   4.  Record Count\n",
    "2. Data Cleaning\n",
    "   1. Checking for duplicates\n",
    "   2. Dropping columns\n",
    "   3. Handling missing values\n",
    "   4. Checking and converting the types\n",
    "3. Defining & Answering the Questions\n",
    "   1. Defining the questions\n",
    "4. Building a Model\n",
    "5. Testing the Model\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **1. Exploring the Dataset**\n",
    "\n",
    "My primary objective is to gain a comprehensive understanding of the dataset that forms the basis of my analysis. I seek answers to the following overarching questions:\n",
    "\n",
    "By addressing these questions, I lay the foundation for a comprehensive exploration of the dataset, enabling me to proceed with informed data analysis and manipulation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Column Identification and Count\n",
    "\n",
    "The first step involves identifying and quantifying the columns present in the dataset. I aim to determine the total number of columns, providing a foundational understanding of the dataset's structure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_df = pd.read_csv('./data/train.csv')\n",
    "\n",
    "print(titanic_df.columns)\n",
    "print(len(titanic_df.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Data Snapshot\n",
    "\n",
    "To grasp the nature of the data, I take a closer look at its content. This involves examining a snapshot of the dataset to get a sense of the actual values and how they are structured:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. Data Types Overview\n",
    "\n",
    "Understanding the types of data present in the dataset is crucial for subsequent processing. I delve into the data types to comprehend the nature of the information:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(titanic_df.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4. Record Count\n",
    "\n",
    "The size of the dataset, measured by the number of records it contains, is an essential factor in understanding its scope. I ascertain the total count of records to gauge the volume of data I'm working with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_entries = titanic_df.shape[0]\n",
    "print(\"Number of records:\", num_entries)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2. Data Cleaning**\n",
    "\n",
    "In the cleaning process, I perform some data checking and manipulation tasks that will hopefully improve the usability and quality of the predictions in later steps. Each dataset is unique, so there are no overarching rules that can always be applied. It is therefore necessary to select the appropriate steps according to the dataset in question.\n",
    "\n",
    "These possible steps include\n",
    "- Checking and removing duplicate data\n",
    "- Possibly dropping unnecessary columns\n",
    "- Dealing with missing values\n",
    "- Checking and possibly converting types\n",
    "- Possibly adjusting the granularity of the data\n",
    "- Find outliers and deal with them appropriately\n",
    "- Feature engineering to create new features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First I create a deep copy to guarantee that the original dataframe stays as it is\n",
    "t_df = titanic_df.copy(deep=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Checking for duplicates\n",
    "\n",
    "First we sort the data, then we check for duplicates. It is important to sort the data first because the `duplicated()` method only compares each row with the previous row. So if there are rows between duplicates, the duplicated() method will not catch them.\n",
    "\n",
    "For the sort values I have chosen the **Ticket** and **Cabin** features:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking all rows from the original DF for duplicates\n",
    "t_df_sorted = t_df.sort_values(by=['Ticket', 'Cabin'])\n",
    "duplicates = t_df_sorted.duplicated()\n",
    "number_duplicate = duplicates.sum()\n",
    "print(\"Number of duplicates:\",number_duplicate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There do not appear to be any duplicates, so we will proceed to the next step. Please note that this is only a very rudimentary technique for checking for duplicate values. There are certainly more sophisticated methods that may be required for more critical data analysis projects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Dropping columns\n",
    "\n",
    "As each row already has an **ID** provided by the dataframe, the **PassengerID** column is redundant and can be dropped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DF with dropped PassengerId column\n",
    "t_df_dropped = t_df.drop(['PassengerId'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. Handling missing values\n",
    "\n",
    "#### **2.3.1. What is missing?**\n",
    "\n",
    "The very first step is to identify where the missing values are situated. It's important to understand why the values are missing. Is there a pattern in the missing data? Is it missing completely at random, or is there a systematic reason? \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "plt.rcParams['font.family'] = \"Verdana\"\n",
    "\n",
    "sns.heatmap(t_df_dropped.isnull(), cmap=\"binary\", cbar=False, yticklabels=50)\n",
    "plt.title(\"Missing Values Heatmap\", loc=\"left\", fontname=\"Verdana\", fontsize=16)\n",
    "plt.yticks(rotation=0)\n",
    "\n",
    "plt.figtext(0.125, 0, \"Black means the value in that column is missing for that passenger.\", ha=\"left\",  fontname=\"Verdana\", fontsize=11, color='#404040')\n",
    "sns.despine(left=False, bottom=False)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE:** Dropping all the rows with missing values results in a dataframe with only 183 rows compared to the original 891. Due to the significance of this action, this approach is not recommended.\n",
    "\n",
    "After analysing the occurrence of missing data, it is clear that there are two main columns in question: **Age** and **Cabin**. \n",
    "Embarked also has some missing values. There are several options: \n",
    "\n",
    "- If a column has a high percentage of missing values and doesn't provide significant information, it can be considered for removal.\n",
    "- If the missing values are limited to a small number of rows, you could consider removing those rows. However, be careful as this may result in the loss of valuable data.\n",
    "- Fill the column with wildcards, for example \"Unknown\", rather than leaving it as NaN or null.\n",
    "- Imputation, where missing values are replaced with estimated values (e.g. mean, median, mode).\n",
    "- Creating multiple versions of the dataset, where missing values are imputed differently in each version. This can help to capture the uncertainty around the imputed values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **2.3.2. Which of the missing values could be important?**\n",
    "\n",
    "We want to find out if there are any features that might be more important than others. We compare these features with the **Survived** feature to see if there are any potential trends. If there is evidence of trends, we may be able to prioritise working on these features and spend more time cleaning and imputing missing values for them to ensure that their quality is good after the cleaning process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **a) The Embarked feature**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_missing_embarked_values = t_df_dropped[\"Embarked\"].isna().sum()\n",
    "print(number_of_missing_embarked_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are only two values missing for the **Embarked** feature. This is a negligible amount and can therefore be considered as unimportant (in my personal interpreation)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **b) The Cabin feature**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a categorical plot\n",
    "plt.rcParams['font.family'] = \"Verdana\"\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.title(\"Distribution of Passenger Classes by Deck\", loc=\"left\", font=\"Verdana\", fontsize=16)\n",
    "ax = sns.countplot(x='Cabin', data=t_df_dropped, hue='Survived', palette=\"Blues_r\")\n",
    "ax.legend(labels=[\"Died\", \"Survived\"], title=\"Survival\")\n",
    "ax.set_ylabel(\"Counts of passengers\")\n",
    "ax.set_xticklabels([])\n",
    "ax.set_xticks([])\n",
    "\n",
    "sns.despine(left=False, bottom=False)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In its current form the **Cabin** feature can not give us any meaningful insights into the survivability of a passenger. The data is too sporadic to make any meaningful assumptions about the influence of the **Cabin** on the survivability.\n",
    "\n",
    "It is also highly unlikely that we are going to be able to precisely impute the matching, missing cabin numbers for all the passenger. A more sensible approach might therefore be to bin the cabins numbers into a new feature called **Deck**. We will explore this path in the following segments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **c) The Age feature**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 1, figsize=(10, 10))\n",
    "\n",
    "plt.rcParams['font.family'] = \"Verdana\"\n",
    "\n",
    "fig.suptitle(\"Passenger Survival Analysis by Age\", fontsize=16)\n",
    "\n",
    "# Plot 1\n",
    "ax1 = sns.countplot(x='Survived', data=t_df_dropped, hue=\"Age\", palette=['black'], ax=axes[0])\n",
    "ax1.set_title(\"Survival distribution by age\", loc=\"left\")\n",
    "ax1.set_ylabel(\"Counts of passengers\")\n",
    "ax1.set_xlabel(\"Survival\")\n",
    "ax1.set_xticklabels([\"Died\", \"Survived\"])\n",
    "ax1.get_legend().remove()\n",
    "fig.text(0.125, 0.475, \"Figure 1: The general distribution of ages for passenger who have either survived or died.\", ha=\"left\", fontname=\"Verdana\", fontsize=11, color='#404040')\n",
    "\n",
    "# Plot 2\n",
    "custom_palette = {0: \"#FF6B6B\", 1: \"#CCE5A8\"} \n",
    "ax2 = sns.countplot(x='Age', data=t_df_dropped, hue=\"Survived\", palette=custom_palette, ax=axes[1])\n",
    "ax2.set_title(\"Sruvival comparison by age\", loc=\"left\")\n",
    "ax2.set_ylabel(\"Counts of passengers\")\n",
    "ax2.set_xlabel(\"Age\")\n",
    "custom_xtick_labels = ['0', '10', '20', '30', '40', '50', '60', '70', '80', '90']\n",
    "ax2.set_xticks(range(0, len(custom_xtick_labels) * 10, 10))\n",
    "ax2.set_xticklabels(custom_xtick_labels)\n",
    "ax2.legend(labels=[\"Died\", \"Survived\"], loc=\"upper right\", title=\"Survival\")\n",
    "fig.text(0.125, 0.025, \"Figure 2: A direct comparison of the survivability of the different ages.\", ha=\"left\",  fontname=\"Verdana\", fontsize=11, color='#404040')\n",
    "\n",
    "sns.despine(left=False, bottom=False)\n",
    "plt.tight_layout(pad=4.0)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At first sight, **Age** feature might play a role in survivability. It might as well also be a proxy for another feature, for example passengers of particular classes might be more prone to belong in a specific age range etc. \n",
    "\n",
    "Anyway, additional care should be taken to ensure that missing values are replaced/imputed accordingly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **2.3.3. How to deal with the missing data**\n",
    "\n",
    "One way of dealing with missing values would be to infer the data from similar entries. For example, suppose we have a passenger with a missing value for cabin:\n",
    "\n",
    "- We could try to find out if they are married to another passenger for whom we know the cabin by checking the name.\n",
    "- We could check if the class gives us a rough estimate of where the cabin would be on the ship, or at least what type of cabin it is.\n",
    "- We could check the price to infer what type of cabin it might be and where it would be located.\n",
    "- We could check if there are any patterns related to the *embarked* feature, i.e. if people who embarked at the same location were allocated similarly located cabins.\n",
    "\n",
    "While this may sound sensible, it is by no means a hard and fast rule and could therefore make the model perform worse. It is therefore a good idea to split the datasets into two versions: one where all entries with missing values are removed, and one where the missing values are inferred from other features.\n",
    "\n",
    "It is also important to think about what we are actually trying to find out and whether the data is relevant to answering those questions. For further elaboration of the questions to be answered, see the section *Definition of the questions to be answered*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean = t_df_dropped.copy(deep=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **a) Missing Embarked values**\n",
    "\n",
    "Since the rows with missing values are limited to a small number of rows in the *Embarked* column, I am going to remove them altogether since there is no clear way of figuring out the values. One might argue, that the *Embarked* column might not be that relevant altogether and it might be more valuable to keep the two entries and just be done with the *Embarked* column, which is a valid argument. However at the time being we will focus on one path, and might come back later to try and optimise the quality of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping the rows with missing values in the Embarked column\n",
    "df_clean = df_clean.dropna(subset=['Embarked'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **b) Missing Cabin values**\n",
    "\n",
    "Given that cabins are organized by class and are likely grouped together on specific decks, it could be advantageous to adjust the granularity of the dataset and introduce a \"deck\" feature. This feature could serve as a substitute for the potentially challenging task of pinpointing individual cabin numbers through a binning method.\n",
    "\n",
    "We delve into this idea because the location of cabins, or the decks they are on, might play a role in the passengers' chances of survival. For instance, proximity to emergency escape routes (or emergency vessels) or distance from the waterline could be critical factors.\n",
    "\n",
    "If classes are indeed grouped within specific decks, we can assign passengers with unknown decks based on the most probable deck associated with their class. This approach can help us piece together valuable information about passenger deck location, ultimately contributing to a more comprehensive analysis of survival factors.\n",
    "\n",
    "To test the assumptions about the class groupings in decks, it is possible to refer to external data like the image below:\n",
    "\n",
    "<img src=\"https://clickamericana.com/wp-content/uploads/White-Star-Line-Titanic-and-Olympic.jpg\" alt=\"Titanic Decks\" width=\"350\"/>\n",
    "\n",
    "(Source: https://clickamericana.com/wp-content/uploads/White-Star-Line-Titanic-and-Olympic.jpg)\n",
    "\n",
    "We can infer from the image that the following deck-class distribution holds:\n",
    "\n",
    "- Deck A:  \n",
    "  - 1st class staterooms\n",
    "- Deck B: \n",
    "  - 1st class staterooms\n",
    "  - 1st class suites\n",
    "  - 1st class cabins\n",
    "- Deck C: \n",
    "  - 1st class apartments\n",
    "  - 1st class cabins de luxe\n",
    "- Deck D: \n",
    "  - 2nd class staterooms\n",
    "  - 3rd class rooms\n",
    "- Deck E:\n",
    "  - 1st class staterooms\n",
    "  - 2nd class staterooms\n",
    "  - 3rd class rooms\n",
    "- Deck F:\n",
    "  - 2nd class staterooms\n",
    "  - 3rd class rooms\n",
    "- Deck G:\n",
    "  - 3rd class rooms\n",
    "\n",
    "\n",
    "First let us create a placeholder value for all the unknown cabin values to make sure there are no Null or NaN values left:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replacing the missing values with 'Unknown' so that they do not contain NaN or null\n",
    "df_clean.loc[df_clean['Cabin'].isnull(), 'Cabin'] = 'Unknown'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then create a **Deck** feature from the **Cabin** feature/ values. The cabin number always starts with a designation letter of the deck. We can slice this character off and insert it into our newly created Deck column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a deck column with the first letter of the cabin\n",
    "df_clean.loc[:, 'Deck'] = df_clean['Cabin'].apply(lambda cabin: cabin[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After creating the **Deck** feature, we can create a plot to get a more comprehensive overview of the distribution of passengers per deck:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['font.family'] = \"Verdana\"\n",
    "plt.figure(figsize=(8, 4))\n",
    "\n",
    "plt.title(\"Distribution of Passenger Classes by Deck\", loc=\"left\", font=\"Verdana\", fontsize=16)\n",
    "ax = sns.countplot(x='Deck', data=df_clean, hue='Pclass', palette=\"Blues_r\", order=sorted(df_clean['Deck'].unique()))\n",
    "ax.set_ylabel(\"Counts of passengers\")\n",
    "ax.legend(title=\"Class\")\n",
    "\n",
    "sns.despine(left=False, bottom=False)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see the *U* (Unknown) deck takes up the vast majority of the passengers, which makes sense, since it was inferred to the **Cabin** feature, which has a lot of missing values. We can filter out the *U* deck so that it does not skew up our visualisation too much and to get a better overview of the count of passengers per deck:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df = df_clean[df_clean['Deck'] != 'U']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['font.family'] = \"Verdana\"\n",
    "plt.figure(figsize=(8, 4))\n",
    "\n",
    "plt.title(\"Counts of Passengers per Deck\", loc=\"left\", font=\"Verdana\", fontsize=16)\n",
    "ax = sns.countplot(x='Deck', data=filtered_df, palette=\"Blues_r\", order=sorted(filtered_df['Deck'].unique()))\n",
    "ax.set_ylabel(\"Counts of passengers\")\n",
    "\n",
    "sns.despine(left=False, bottom=False)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we could try to find out if there are particular classes that belong to decks. We can then compare our findings with the deck distribution that we got from the image bove.\n",
    "\n",
    "Knowing which classes belong to which decks could possibly allow us to infer the missing deck value for passenger for whom we know the class, but not the deck. To do this, let us first visualize the distribution of classes per deck and see if we can learn something from the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['font.family'] = \"Verdana\"\n",
    "plt.figure(figsize=(8, 4))\n",
    "\n",
    "plt.title(\"Distribution of Passenger Classes by Deck (without Unknowns)\", loc=\"left\", font=\"Verdana\", fontsize=16)\n",
    "ax = sns.countplot(x='Deck', data=filtered_df, hue='Pclass', palette=\"Blues_r\", order=sorted(filtered_df['Deck'].unique()))\n",
    "ax.set_ylabel(\"Counts of passengers\")\n",
    "ax.legend(title=\"Class\")\n",
    "\n",
    "sns.despine(left=False, bottom=False)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Findings just based on visual analysis:\n",
    "\n",
    "- Deck A only 1st class passengers\n",
    "- Deck B only 1st class passengers\n",
    "- Deck C only 1st class passengers\n",
    "- Deck D first mixed passengers class deck (1st & 2nd)\n",
    "- Deck E all classes\n",
    "- Deck F only 2nd and 3rd class passengers\n",
    "- Deck G exclusively 3rd class passengers\n",
    "- Deck T only first class passengers (possibly a special deck, due to the minuscule amount and only 1st class)\n",
    "\n",
    "This matches the distribution that we inferred from the image above, and allows us to make more generalized assumptions about what type of passengers to expect on which decks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **c) Missign Age values**\n",
    "\n",
    "Since the age plays a vital role in survivability in general, it makes sense to spend a little bit more time on trying to fill in the gaps.\n",
    "\n",
    "Here are some techniques that we will explore to infer the values ang create different forks of the dataset:\n",
    "\n",
    "- I) Randomly assign plausible age ranges to unknown values.\n",
    "- II) Use linear regression based on other relevant features like passenger class, gender, and family size.\n",
    "- III) Apply K-Nearest Neighbors (KNN) imputation.\n",
    "- IV) Utilize Bayesian Imputation techniques.\n",
    "\n",
    "There certainly are other and more exhaustive techniques, but in the scope of this project, I will solemnly focus on the previously mentioned techniques. These different datasets can then be used to create different models which we then can test to find the best performing one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Transforming the dataset for further exploration\n",
    "\n",
    "First, let us create a new deep copy of the existing dataset and adapt the granularity of the **Name** feature and split it into \"Last Name,\" \"Title,\" and \"First Name\". After this we will be binning (creating ranges) the **Age**, because estimating the exact age per person is pretty difficult."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "age_cleaning = df_clean.copy(deep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the \"Name\" feature into \"Last Name,\" \"Title,\" and \"First Name\"\n",
    "name_split = age_cleaning['Name'].str.split(',', n=1, expand=True)\n",
    "age_cleaning.loc[:, 'Last Name'] = name_split[0].str.strip()\n",
    "age_cleaning.loc[:, 'Rest'] = name_split[1]\n",
    "\n",
    "title_split = age_cleaning['Rest'].str.split('.', n=1, expand=True)\n",
    "age_cleaning.loc[:, 'Title'] = title_split[0].str.strip()\n",
    "age_cleaning.loc[:, 'First Name'] = title_split[1].str.strip()\n",
    "\n",
    "# Removing extra whitespace from columns\n",
    "age_cleaning.loc[:, 'Last Name'] = age_cleaning['Last Name'].str.strip()\n",
    "age_cleaning.loc[:, 'First Name'] = age_cleaning['First Name'].str.strip()\n",
    "\n",
    "age_cleaning.drop(columns=['Rest'], inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "age_ranges = [\n",
    "    (0, 5, '0-5'),\n",
    "    (6, 12, '6-12'),\n",
    "    (13, 19, '13-19'),\n",
    "    (20, 29, '20-29'),\n",
    "    (30, 39, '30-39'),\n",
    "    (40, 49, '40-49'),\n",
    "    (50, 59, '50-59'),\n",
    "    (60, 69, '60-69'),\n",
    "    (70, 79, '70-79'),\n",
    "    (80, float('inf'), '80+')\n",
    "]\n",
    "\n",
    "# Function to map age to age range label\n",
    "def map_age_to_range(age):\n",
    "    for start, end, label in age_ranges:\n",
    "        if start <= age <= end:\n",
    "            return label\n",
    "    return 'Unknown'\n",
    "\n",
    "# Creating the ARange column by applying the map_age_to_range function to the Age column\n",
    "age_cleaning.loc[:,'ARange'] = df_clean['Age'].apply(lambda age: map_age_to_range(age) if pd.notnull(age) else 'Unknown')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following heatmap plot provides a visual representation of how different titles (e.g., Mr., Mrs., Miss, etc.) are distributed across various age ranges/ groups. This can help understand the demographics of passengers on the Titanic more clearly.We might also be able to identify possible patterns and trends in the age distribution of passengers based on their titles. This can be valuable for exploring questions like, \"Are certain titles associated with specific age groups?\" or \"How does the age distribution vary among different titles?\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df = age_cleaning[age_cleaning['ARange'] != 'Unknown']\n",
    "cross_tab = pd.crosstab(age_cleaning['Title'], filtered_df['ARange'])\n",
    "\n",
    "plt.rcParams['font.family'] = \"Verdana\"\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "sns.heatmap(cross_tab, annot=True, fmt='d', cmap='Blues', cbar=False)\n",
    "\n",
    "plt.title(\"Occurrences of Titles in Different Age Groups\", loc=\"left\", fontname=\"Verdana\", fontsize=16)\n",
    "plt.xlabel(\"Age Range\")\n",
    "plt.ylabel(\"Title of the passenger\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **c) I. Randomly assign plausible age ranges to unknown values**\n",
    "\n",
    "The next thing we can do is to calculate the probability of people belonging to a certain age group based on their titles (while ignoring entries with \"Unknown\" in their ARange feature).\n",
    "\n",
    "Based on these probabilities we can randomly assign the passengers with missing age values (or age range values) their ages based on their titles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df_only_unkowns = age_cleaning[age_cleaning['ARange'] == 'Unknown']\n",
    "grouped_unkowns = filtered_df_only_unkowns.groupby('Title').size().reset_index(name='total_count')\n",
    "\n",
    "grouped = filtered_df.groupby(['Title', 'ARange']).size().reset_index(name='count')\n",
    "total_counts = filtered_df.groupby('Title').size().reset_index(name='total_count')\n",
    "\n",
    "arange_probabilities = pd.merge(grouped, total_counts, on='Title')\n",
    "arange_probabilities['probability'] = arange_probabilities['count'] / arange_probabilities['total_count']\n",
    "arange_probabilities['probability_percent'] = (arange_probabilities['count'] / arange_probabilities['total_count']) * 100\n",
    "\n",
    "arange_probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us have at what passengers (i.e. types of titles) have missing age values. Based on these sums we can use the probabilities for these titles to calculate the amount of age ranges that need to be randomly distributed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_unkowns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_probabilites_for_title (title):\n",
    "    index = 0\n",
    "    amount_of_groups = arange_probabilities[arange_probabilities['Title'] == title].shape[0]\n",
    "    amount_titles_with_missing_values = grouped_unkowns[grouped_unkowns['Title'] == title]['total_count'].values[0]\n",
    "    while amount_of_groups > 0:\n",
    "        print(round(arange_probabilities[arange_probabilities['Title'] == title]['probability'].values[index] * amount_titles_with_missing_values),\" \", arange_probabilities[arange_probabilities['Title'] == title]['ARange'].values[index])\n",
    "        index += 1\n",
    "        amount_of_groups -= 1\n",
    "\n",
    "get_probabilites_for_title(\"Master\")\n",
    "\n",
    "# now randomly assign plausible age ranges to unknown values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "age_probability_guess = age_cleaning.copy(deep=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **c) II. Linear Regression**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **c) III. K-Nearest Neighbour (KNN)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **c) IV. Bayesian Imputation**\n",
    "\n",
    "* Prior Distribution: Define a prior distribution for the ages based on any available domain knowledge or information. This could be a distribution that represents your beliefs about the likely age distribution of passengers.\n",
    "* Likelihood Model: Construct a likelihood model that describes the relationship between the available information (e.g., passenger class, gender, title) and age. You can use probabilistic models such as Bayesian regression or Bayesian networks for this.\n",
    "* Bayesian Inference: Apply Bayesian inference techniques, such as Markov Chain Monte Carlo (MCMC) or Variational Inference, to update the prior distribution based on the observed data. In this case, the observed data would be the non-missing age values and the features that may influence age.\n",
    "* Posterior Distribution: After performing Bayesian inference, you'll obtain a posterior distribution of ages for each passenger with missing age values.\n",
    "* Imputation: Use the posterior distribution to sample or estimate imputed age values for passengers with missing data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **2.3.4. Sanity Check**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a heatmap of missing values to check if none are still missing\n",
    "plt.rcParams['font.family'] = \"Verdana\"\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "title = plt.title(\"Missing Values Heatmap (after cleaning)\", loc=\"left\", fontname=\"Verdana\", fontsize=16)\n",
    "\n",
    "sns.heatmap(age_probability_guess.isnull(), cmap=\"binary\", cbar=False, yticklabels=100)\n",
    "plt.yticks(rotation=0)\n",
    "plt.xticks(rotation=45)\n",
    "sns.despine(left=False, bottom=False)\n",
    "\n",
    "description = \"Black means the value in that column is missing for that passenger.\"\n",
    "plt.figtext(0.125, -0.1, description, ha=\"left\",  fontname=\"Verdana\", fontsize=11, color='#404040')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4. Checking and converting the types\n",
    "\n",
    "To male operations more effictient it is recommended effectively convert the types to the smallest possible data types. For example the 'Survived' feature is comprised of only 0 and 1, therefore it does not make sense for it to be an INT64 data type. A much more efficient type would be the boolean data type. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "converted_cleaned_df = age_probability_guess.copy(deep=True)\n",
    "\n",
    "# Type conversion of DF with removed values\n",
    "converted_cleaned_df['Survived'] = converted_cleaned_df['Survived'].astype('int8')\n",
    "converted_cleaned_df['Pclass'] = converted_cleaned_df['Pclass'].astype('int8')\n",
    "converted_cleaned_df['Name'] = converted_cleaned_df['Name'].astype('string')\n",
    "converted_cleaned_df['Last Name'] = converted_cleaned_df['Last Name'].astype('string')\n",
    "converted_cleaned_df['First Name'] = converted_cleaned_df['First Name'].astype('string')\n",
    "converted_cleaned_df['Embarked'] = converted_cleaned_df['Embarked'].astype('string')\n",
    "converted_cleaned_df['Cabin'] = converted_cleaned_df['Cabin'].astype('string')\n",
    "converted_cleaned_df['Ticket'] = converted_cleaned_df['Ticket'].astype('string')\n",
    "converted_cleaned_df['Title'] = converted_cleaned_df['Title'].astype('string')\n",
    "converted_cleaned_df['Deck'] = converted_cleaned_df['Deck'].astype('string')\n",
    "converted_cleaned_df['Sex'] = converted_cleaned_df['Sex'].astype('string')\n",
    "converted_cleaned_df['SibSp'] = converted_cleaned_df['SibSp'].astype('int8')\n",
    "converted_cleaned_df['Parch'] = converted_cleaned_df['Parch'].astype('int8')\n",
    "\n",
    "\n",
    "print(converted_cleaned_df.dtypes)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **3. Defining & Answering the Questions**\n",
    "\n",
    "The overall question is: What are the main factors that contributed to the survival rate of a passenger on the Titanic?\n",
    "\n",
    "### 3.1. Defining the questions\n",
    "\n",
    "To build a successful model that attempts to predict survivability, it is first necessary to find related features. To find possible features that affect survivability, I first define some assumptions that I want to test:\n",
    "\n",
    "**1. Does class have an effect on the survivability?**\n",
    "\n",
    "Assumption: due to the fact that higher class passengers had priority on emergency ships\n",
    "\n",
    "**2. Does age have an effect on the survivability?** \n",
    "\n",
    "Assumption: due to the fitness of the individual, there could be an advantage.\n",
    "\n",
    "**3. Does the amount of parents / childen (Parch) have an effect on the survivability?** \n",
    "\n",
    "Assumption: family size may help identify passengers who were traveling with dependent family members, which in turn might have influenced each other's survival decisions.\n",
    "\n",
    "**4. Does the amount of spouses / siblings (SibSp) have an effect on the survivability?** \n",
    "\n",
    "Assumption: passengers with more siblings or spouses on board might have had different survival strategies or outcomes compared to those traveling alone.\n",
    "\n",
    "**5. Does the cabin, and threfore deck, have an effect on the survivability?**\n",
    "\n",
    "Assumption: due to proximity to exits and/ or proxy of their class.\n",
    "\n",
    "Please note that these assumptions are completely made up and based on intuition. They are just there to give me something to work with and investigate. Even if there is a relationship between survivability and the feature, this does not necessarily mean that there is a correlation or proof of my initial assumption."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Trying to answer the questions\n",
    "\n",
    "#### **3.2.1. The role of class**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a figure with two subplots\n",
    "fig, axes = plt.subplots(3, 2, figsize=(14, 12))\n",
    "plt.rcParams['font.family'] = \"Verdana\"\n",
    "\n",
    "fig.suptitle(\"Indicators for survival - class relation\", fontsize=18, fontweight=\"bold\")\n",
    "\n",
    "# First subplot - Survival rates\n",
    "plt.sca(axes[0, 0])\n",
    "plt.title(\"Survival rates of passengers per place of embarkment\", loc=\"left\", font=\"Verdana\", fontsize=16)\n",
    "ax1 = sns.countplot(x='Embarked', data=t_df_dropped, hue=\"Survived\", palette=\"Blues_r\")\n",
    "ax1.set_ylabel(\"Counts of passengers\")\n",
    "ax1.set_xticklabels([\"Southampton\", \"Cherbourg\", \"Queenstown\"])\n",
    "ax1.legend(title=\"Survival\",labels=[\"Died\", \"Survived\"])\n",
    "\n",
    "# Second subplot - Passenger classes\n",
    "plt.sca(axes[0, 1])\n",
    "plt.title(\"Ports of embarkment in relation to passenger classes\", loc=\"left\", font=\"Verdana\", fontsize=16)\n",
    "ax2 = sns.countplot(x='Embarked', data=t_df_dropped, hue=\"Pclass\", palette=\"Blues_r\")\n",
    "ax2.set_ylabel(\"Counts of passengers\")\n",
    "ax2.set_xticklabels([\"Southampton\", \"Cherbourg\", \"Queenstown\"])\n",
    "ax2.legend(title=\"Class\")\n",
    "\n",
    "# Third subplot - Passenger genders\n",
    "plt.sca(axes[1, 0])\n",
    "plt.title(\"Mortality rate by gender\", loc=\"left\", font=\"Verdana\", fontsize=16)\n",
    "ax3 = sns.countplot(x='Sex',data=t_df_dropped, hue=\"Survived\", palette=\"Blues_r\")\n",
    "ax3.set_ylabel(\"Counts of passengers\")\n",
    "ax3.legend(title=\"Survival\",labels=[\"Died\", \"Survived\"])\n",
    "\n",
    "# Fourth subplot - Passenger gender per class\n",
    "plt.sca(axes[1, 1])\n",
    "plt.title(\"Survival rates of passengers on the Titanic per class\", loc=\"left\", font=\"Verdana\", fontsize=16)\n",
    "ax4 = sns.countplot(x='Sex', data=t_df_dropped, hue=\"Pclass\", palette=\"Blues_r\")\n",
    "ax4.set_ylabel(\"Counts of passengers\")\n",
    "ax4.legend(title=\"Class\")\n",
    "\n",
    "# Fifth subplot - Passenger survival rate by class\n",
    "plt.sca(axes[2, 0])\n",
    "plt.title(\"Survival rates of passengers per class\", loc=\"left\", font=\"Verdana\", fontsize=16)\n",
    "ax5 = sns.countplot(x='Survived', data=t_df_dropped, hue=\"Pclass\", palette=\"Blues_r\")\n",
    "ax5.set_ylabel(\"Counts of passengers\")\n",
    "ax5.set_xticklabels([\"Died\", \"Survived\"])\n",
    "ax5.legend(title=\"Class\")\n",
    "\n",
    "# Sixth subplot - General passenger survival rate\n",
    "plt.sca(axes[2, 1])\n",
    "plt.title(\"General survival rate of all the passengers\", loc=\"left\", font=\"Verdana\", fontsize=16)\n",
    "ax6 = sns.countplot(x='Survived', data=t_df_dropped, palette=\"Blues_r\")\n",
    "ax6.set_ylabel(\"Counts of passengers\")\n",
    "ax6.set_xlabel(\"Survival\")\n",
    "ax6.set_xticklabels([\"Died\", \"Survived\"])\n",
    "\n",
    "# Adjust spacing between subplots\n",
    "plt.tight_layout()\n",
    "sns.despine(left=False, bottom=False)\n",
    "\n",
    "# Show the combined plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the total number of passengers\n",
    "total_passengers = len(t_df_dropped)\n",
    "not_survived_count = len(t_df_dropped[t_df_dropped['Survived'] == 0])\n",
    "overall_mortality_rate = not_survived_count / total_passengers\n",
    "print(\"Overall Mortality Rate:\", overall_mortality_rate)\n",
    "\n",
    "# First, let's calculate the total number of passengers in each class\n",
    "class_counts = t_df_dropped['Pclass'].value_counts().sort_index()\n",
    "not_survived_counts = t_df_dropped[t_df_dropped['Survived'] == 0]['Pclass'].value_counts().sort_index()\n",
    "mortality_rate = not_survived_counts / class_counts\n",
    "print(\"Mortality Rate by\", mortality_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upon examining the graphs, it may initially appear that the port of embarkation and gender have a significant impact on survivability. However, a deeper analysis reveals that these factors are proxies for a more influential predictor: passenger class.\n",
    "\n",
    "For instance, passengers embarking from Southampton had a higher likelihood of not surviving compared to those from other ports. This observation becomes more enlightening when considering the composition of Southampton passengers, who were predominantly third-class travelers.\n",
    "\n",
    "Similarly, the data suggests that the mortality rate among men is higher than that among women. Once again, this can be elucidated by examining the passenger composition. Males were notably more likely to belong to the third-class category.\n",
    "\n",
    "In a broader context, third-class passengers exhibited a mortality rate of 75.7%, while first-class passengers had a substantially lower rate of 37.0%. These figures significantly deviate from the overall mortality rate for all passengers, which stands at 61.6%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **3.2.2. The role of Age**\n",
    "\n",
    "\n",
    "**Notable:**\n",
    "- very young passengers seemed to do better\n",
    "- Very old passengers seemed to do worse. \n",
    "- Mortality in the 20-40 age group seemed to be higher than in other age groups."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **3.2.3. The role of Parents / Children (Parch)**\n",
    "\n",
    "The **Parch** feature expresses the number of parents / children on board. \n",
    "\n",
    "The dataset defines family relations in this way: \n",
    "- Parent = mother, father\n",
    "- Child = daughter, son, stepdaughter, stepson\n",
    "\n",
    "Some children travelled only with a nanny, therefore parch=0 for them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "plt.rcParams['font.family'] = \"Verdana\"\n",
    "\n",
    "fig.suptitle(\"Parch Group Distribution and Mortality Analysis\", fontsize=16)\n",
    "\n",
    "# Plot 1\n",
    "ax1 = sns.countplot(x='Parch',data=t_df_dropped, palette=\"Blues_r\", ax=axes[0])\n",
    "ax1.set_title(\"Distribution of different Parch groups\", loc=\"left\")\n",
    "ax1.set_ylabel(\"Counts of passengers\")\n",
    "ax1.set_xlabel(\"Number of parents / children (Parch)\")\n",
    "\n",
    "sns.despine(left=False, bottom=False, ax=ax1)\n",
    "\n",
    "# Plot 2\n",
    "ax2.set_title(\"Survival rates of different Parch groups\", loc=\"left\")\n",
    "ax2 = sns.countplot(x='Parch',data=t_df_dropped, hue=\"Survived\",palette=\"Blues_r\", ax=axes[1])\n",
    "ax2.set_ylabel(\"Counts of passengers\")\n",
    "ax2.set_xlabel(\"Number of parents / children (Parch)\")\n",
    "ax2.legend(labels=[\"Died\", \"Survived\"], title=\"Survival\")\n",
    "\n",
    "sns.despine(left=False, bottom=False, ax=ax2)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **3.2.4. The role of Siblings / Spouses (SibSp)**\n",
    "\n",
    "The **SibSP** feature expresses the number of siblings / spouses on board.\n",
    "\n",
    "The dataset defines family relations in this way:\n",
    "- Sibling = brother, sister, stepbrother, stepsister\n",
    "- Spouse = husband, wife (mistresses and fiancés were ignored)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "plt.rcParams['font.family'] = \"Verdana\"\n",
    "\n",
    "fig.suptitle(\"SibSp Group Distribution and Mortality Analysis\", fontsize=16)\n",
    "\n",
    "# Plot 1\n",
    "ax1 = sns.countplot(x='SibSp',data=t_df_dropped, palette=\"Blues_r\", ax=axes[0])\n",
    "ax1.set_title(\"Distribution of different SibSp groups\", loc=\"left\")\n",
    "ax1.set_ylabel(\"Counts of passengers\")\n",
    "ax1.set_xlabel(\"Number of siblings / spouses (SibSp)\")\n",
    "\n",
    "sns.despine(left=False, bottom=False, ax=ax1)\n",
    "\n",
    "# Plot 2\n",
    "ax2 = sns.countplot(x='SibSp',data=t_df_dropped, hue=\"Survived\", palette=\"Blues_r\", ax=axes[1])\n",
    "ax2.set_title(\"SibSp distribution of the passengers\", loc=\"left\")\n",
    "ax2.set_ylabel(\"Counts of passengers\")\n",
    "ax2.set_xlabel(\"Number of siblings / spouses (SibSp)\")\n",
    "ax2.legend(labels=[\"Died\", \"Survived\"], title=\"Survival\")\n",
    "\n",
    "sns.despine(left=False, bottom=False, ax=ax2)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **3.2.5. The role of Cabin (Deck)**\n",
    "\n",
    "Another intersting insight would be to know what the mortality rate looks like per deck:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 4))\n",
    "\n",
    "plt.rcParams['font.family'] = \"Verdana\"\n",
    "\n",
    "custom_palette = {0: \"#FF6B6B\", 1: \"#CCE5A8\"} \n",
    "\n",
    "ax2 = sns.countplot(x='Deck', data=filtered_df, hue='Survived', palette=custom_palette, order=sorted(filtered_df['Deck'].unique()))\n",
    "plt.title(\"Mortality Rate per Deck (without Unknowns)\", loc=\"left\", font=\"Verdana\", fontsize=16)\n",
    "ax2.set_ylabel(\"Counts of passengers\")\n",
    "ax2.legend(title=\"Survival\",labels=[\"Died\", \"Survived\"])\n",
    "\n",
    "sns.despine(left=False, bottom=False)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4",
   "language": "python",
   "name": "python3.10"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
